---
title: "HW#4_Data621"
author: "Saloua Daouki"
date: "2024-11-15"
output:
  pdf_document:
    toc: true
    toc_depth: '3'
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading Necessary Libraries and the data


```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(car)
library(e1071)
library(MASS)
library(pROC)
```

```{r}
insurance_training <- read.csv("insurance_training_data.csv")
insurance_evaluation <- read.csv("insurance-evaluation-data.csv")
```

## 1. Data Exploration:

### a. Mean / Standard Deviation / Median

```{r}
# Quick exploration
str(insurance_training)  # Structure of data
summary(insurance_training)  # Summary statistics

# Check for missing values
colSums(is.na(insurance_training))
```
 
The data has 8161 observations and 25 variables (excluding the `INDEX` which won't be used for the analysis).

The primary target variable is `TARGET_FLAG`, a binary indicator representing whether a car was in crash, and the secondary target `TARGET_AMT` indicates the amount of the cost if a car was in crash.

`AGE` has a mean of 44.8 years (SD = 14.3) with a median age of 45, indicating a balanced age distribution.
`TRAVTIME` (commute time to work) averages 33.5 minutes, with most values clustered between 22 and 44 minutes. A full table of key statistics is included above for reference.

Several variables have missing values:

- `AGE` (6 missing values), `YOJ` (454), `INCOME` (many blanks), and `CAR_AGE` (510).
We are going to apply imputation strategies to address these gaps. Missing AGE values will be replaced with the median (45 years).

- `YOJ` and `CAR_AGE` will be imputed using their median values (11 and 8 years, respectively).
`INCOME`, recorded as character strings, will be cleaned and converted to numeric, with missing values replaced by the median.

### b. Bar Chart or Box Plot of the data

#### b.1. Visualize Numeric Variables

```{r}
# Histogram for AGE
ggplot(insurance_training, aes(x = AGE)) +
  geom_histogram(bins = 30, fill = "skyblue", color = "black") +
  labs(title = "Age Distribution", x = "Age", y = "Frequency")

# Boxplot for CAR_AGE
ggplot(insurance_training, aes(y = CAR_AGE)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Boxplot of CAR_AGE", y = "Car Age")

# Remove rows with missing values
numeric_data <- na.omit(insurance_training[, sapply(insurance_training, is.numeric)])

# Checking correlation between numeric features
numeric_vars <- sapply(insurance_training, is.numeric)
correlation_matrix <- cor(insurance_training[, numeric_vars], use = "pairwise.complete.obs")
print(correlation_matrix)
```
The histogram for the variable `AGE` shows an approximately normal distribution, with the highest frequency occurring around the median age of 45.

For `CAR_AGE`, outliers were evident, with the minimum value being -4, indicating that a negtive age doesn't make sense. 

A correlation matrix highlighted several key relationships:

- The target variable `TARGET_FLAG` has a moderate positive correlation with `MVR_PTS` (Motor Vehicle Record Points, 0.22) and `CLM_FREQ` (Claim Frequency, 0.22). These variables are strong candidates for inclusion in the logistic regression model.

- `AGE` and `CAR_AGE` exhibit weak negative correlations with `TARGET_FLAG` (-0.10 each), suggesting limited predictive value.

- `TARGET_AMT` is moderately correlated with `TARGET_FLAG` (0.53), as expected, since claim amounts/cost depend on whether a claim was filed for a crashed car.


#### b.2. Visualize Categorical Variables:

```{r}
# Bar plot for CAR_TYPE
ggplot(insurance_training, aes(x = CAR_TYPE)) +
  geom_bar(fill = "purple") +
  labs(title = "Car Type Distribution", x = "Car Type", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Bar plot for TARGET_FLAG
ggplot(insurance_training, aes(x = factor(TARGET_FLAG))) +
  geom_bar(fill = "orange") +
  labs(title = "Target Flag Distribution", x = "Target Flag (0 = No Crash, 1 = Crash)", y = "Count")
```
Vehicle Types: The dataset includes a variety of vehicle categories, with the most common being SUVs (Z_SUV), accounting for 2,260 entries, followed by:

- Minivans: 2,020 entries

- Pickup Trucks: 1,375 entries

- Sports Cars: 875 entries

- Vans: 750 entries

- Panel Trucks: 700 entries

This distribution highlights a predominance of family-oriented and utility vehicles, potentially influencing claim tendencies.

Target Flag Distribution: The target variable TARGET_FLAG has a highly imbalanced distribution:

- No Crash (0): 6,000 instances (approximately 73.5% of the data).

- Crash (1): 2,100 instances (approximately 26.5%).

The imbalance will be considered in model development, potentially requiring techniques like weighting, oversampling, or undersampling to ensure accurate prediction.

```{r}
# identify missing values
colSums(is.na(insurance_training))
```


## 2. Data Preparation:

### a. Handling Missing Values:

Since the variables that have missing values are numeric, we are going to impute the missing values using the mean.

```{r}
# Impute missing values for numerical variables with mean
numeric_vars <- sapply(insurance_training, is.numeric)
insurance_training[numeric_vars] <- lapply(insurance_training[numeric_vars], function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
```

### b. Creating Flags for Missing Values:

```{r}
# Loop through all variables to create flags for missing values
for (var in colnames(insurance_training)) {
  insurance_training[paste0(var, "_FLAG")] <- ifelse(is.na(insurance_training[[var]]), 1, 0)
}

# Check the new flags columns
head(insurance_training)
```

- The paste0(var, "_FLAG") dynamically creates the name for the new flag column based on the original variable name (e.g., if the original variable is AGE, the flag column will be AGE_FLAG).

- ifelse(is.na(insurance_training[[var]]), 1, 0) checks if the value is missing (NA), and if it is, it assigns a 1; otherwise, it assigns a 0.

### c. Transforming data by putting it into buckets:

In this sub-section, we are going to bucketize the continuous variables; `AGE` and `TARGET_AMT`:

```{r}
# Bucketize AGE into ranges
insurance_training$AGE_BUCKET <- cut(insurance_training$AGE,
                                breaks = c(18, 30, 50, 70, Inf),
                                labels = c("18-30", "31-50", "51-70", "70+"))

# Bucketize TARGET_AMT into categories
insurance_training$TARGET_AMT_BUCKET <- cut(insurance_training$TARGET_AMT,
                                       breaks = c(0, 1000, 5000, 10000, Inf),
                                       labels = c("0-1000", "1001-5000", "5001-10000", "10000+"))

# Check the bucketized varaibles
table(insurance_training$AGE_BUCKET)
table(insurance_training$TARGET_AMT_BUCKET)
```
By bucketizing `AGE` into discrete categories, it makes the variable easier to interpret and analyze. Similarly, bucketizing `TARGET_AMT` helps transform a continuous variable with potentially high variation into manageable categories. This can help with clearer reporting and analysis of trends.

### d. Mathematical transforms such as log or square root (or use Box-Cox):

First and to have a clear decision about the type of transformation based on the skewness of each variable:

```{r}
# Check skewness for numeric variables
skew_values <- sapply(insurance_training[, c("AGE", "CAR_AGE", "TARGET_AMT", "KIDSDRIV", "HOMEKIDS")], skewness, na.rm = TRUE)

# View skewness values
print(skew_values)
```

Interpretations: 

- `AGE`: -0.03
This value is close to 0, indicating that the AGE variable is approximately normally distributed. No transformation is needed.

- `CAR_AGE`: 0.30
The skewness of CAR_AGE is slightly positive, but it is relatively close to 0, meaning it is only mildly skewed. We may not need a transformation for this variable, as the skewness is not severe. 

- `TARGET_AMT`: 8.71
This is highly positively skewed, with a skewness greater than 1. This suggests that TARGET_AMT has a long right tail, which is typical for monetary data. **A log transformation** would be helpful in normalizing this variable.

- `KIDSDRIV`: 3.35
This has significant positive skewness, but it’s not extreme. If you want to reduce the skewness, you could consider a log transformation, but it might not be absolutely necessary if the model can handle the skewness well.

- `HOMEKIDS`: 1.34
This value also indicates mild positive skewness. Similar to CAR_AGE, no transformation is strictly necessary, but a log transformation could slightly improve the distribution, especially if we are aiming for perfect normality.


Now, based on the skewness above, we only need to log-transform the `TARGET_AMT`, and the other two variables that have a slight high skewness:

```{r}
# Apply log transformation to TARGET_AMT amd the others
insurance_training$TARGET_AMT_LOG <- log(insurance_training$TARGET_AMT + 1)

insurance_training$KIDSDRIV_LOG <- log(insurance_training$KIDSDRIV + 1)
insurance_training$HOMEKIDS_LOG <- log(insurance_training$HOMEKIDS + 1)
```

Let's check the skewness values after the transformations we performed above:

```{r}
# Check skewness after applying the transformations
skew_values_after_transformation <- sapply(insurance_training[, c("AGE", "CAR_AGE", "TARGET_AMT_LOG", "KIDSDRIV_LOG", "HOMEKIDS_LOG")], skewness, na.rm = TRUE)

# View the skewness values after transformation
print(skew_values_after_transformation)
```

That is good progress;

- The log transformation on `TARGET_AMT` has reduced the skewness significantly, but it remains moderately skewed. This is typical for monetary variables. The transformation has improved the distribution but could still benefit from further adjustments.

- The transformation on `KIDSDRIV` has reduced the skewness but it is still quite positive. This suggests that the log transformation helped, but the variable is still somewhat skewed. We should consider another transformation.

- The log transformation on `HOMEKIDS` has reduced the skewness to a more acceptable level, bringing it closer to zero. This variable is now much more normally distributed and ready for modeling.

One additional tranformation that can help us normalize the continuous variable `TARGET_AMT`is Box-Cox Transformation

```{r}
insurance_training$TARGET_AMT_SHIFTED <- insurance_training$TARGET_AMT + 1
boxcox_result <- boxcox(TARGET_AMT_SHIFTED ~ 1, data = insurance_training)
lambda <- boxcox_result$x[which.max(boxcox_result$y)]
insurance_training$TARGET_AMT_BOXCOX <- (insurance_training$TARGET_AMT_SHIFTED^lambda - 1) / lambda
```

we can also perform the square root transformation:

```{r}
insurance_training$TARGET_AMT_SQRT <- sqrt(insurance_training$TARGET_AMT)
```

Let's do the same thing for the variable `KIDSDRIV`:

First, Box-Cox:

```{r}
insurance_training$KIDSDRIV_BOXCOX <- (insurance_training$KIDSDRIV + 1)^lambda - 1
```

Then, we can use Cube Root transformation:

```{r}
insurance_training$KIDSDRIV_CUBE <- sign(insurance_training$KIDSDRIV) * abs(insurance_training$KIDSDRIV)^(1/3)
```

Let's check once more for after-transformations-skewness 

```{r}
# Check skewness after applying the transformations
skew_values_after_transformation2 <- sapply(insurance_training[, c("AGE", "CAR_AGE", "TARGET_AMT_BOXCOX", "KIDSDRIV_BOXCOX", "HOMEKIDS_LOG")], skewness, na.rm = TRUE)

# View the skewness values after transformation
print(skew_values_after_transformation2)
```

```{r}
# Check skewness after applying the transformations
skew_values_after_transformation3 <- sapply(insurance_training[, c("AGE", "CAR_AGE", "TARGET_AMT_SQRT", "KIDSDRIV_CUBE", "HOMEKIDS_LOG")], skewness, na.rm = TRUE)

# View the skewness values after transformation
print(skew_values_after_transformation3)
```

Based on the transformations above:

- `TARGET_AMT`: Box-Cox was more effective in reducing skewness compared to the square root or cube transformations. While for `KIDSDRIV`, Box-Cox made the variable more negatively skewed, whereas cube transformation made it more positively skewed. Neither transformation worked well. So we better keep the _CUBE or find another approach for this variable.

### e. Creating New Variables:

Age-based Grouping (AGE_GROUP): Age is a continuous variable, but for the purposes of analysis and modeling, grouping it into categories allows us to better understand trends in different age ranges. For example, it might be valuable to compare the behavior of individuals in their 20s versus those in their 50s when it comes to claims or risk.

```{r}
# Create age groups
insurance_training$AGE_GROUP <- cut(insurance_training$AGE, 
                               breaks = c(18, 30, 50, Inf), 
                               labels = c("18-30", "31-50", "51+"))
```

Creating Ratio Variable (KIDSDRIV_RATIO):  This gives us a relative measure of how many kids are driving in relation to the parent's age. This might indicate a trend where younger parents might have fewer kids driving or older parents might have more kids in the driving age range. This may impact outcomes like insurance risk or claim amounts.

```{r}
# Create a new variable as the ratio of KIDSDRIV to AGE
insurance_training$KIDSDRIV_RATIO <- insurance_training$KIDSDRIV / insurance_training$AGE
```

```{r}
# Create a new variable as the ratio of HOMEKIDS to AGE
insurance_training$HOMEKIDS_RATIO <- insurance_training$HOMEKIDS / insurance_training$AGE
```


## 3. BUILD MODELS:

### 3.1 Multiple Linear Regression Models:

#### 3.1.1 Model 1: Using original varaibles

We are going to use the variables; AGE, CAR_AGE, KIDSDRIV_LOG, HOMEKIDS_LOG which are likely to impact the target variable. We use log-transformed TARGET_AMT to handle skewness.

```{r}
# Multiple Linear Regression - Model 1 (using selected transformed variables)
model1 <- lm(TARGET_AMT_LOG ~ AGE + CAR_AGE + KIDSDRIV_LOG + HOMEKIDS_LOG, data = insurance_training)
summary(model1)
```

The model above explains only about 2.67% of the variance in the target variable (
$R^2=0.0267$), which suggests that while the predictors are statistically significant, they do not account for much of the variability in the target variable.

The significant predictors are AGE, CAR_AGE, KIDSDRIV_LOG, and HOMEKIDS_LOG, with the strongest positive relationship seen in KIDSDRIV_LOG and HOMEKIDS_LOG, while AGE and CAR_AGE have negative relationships with the target variable.

#### 3.1.2. Model 2: Using Transformed Variables

In this model, we’ll use the log-transformed variables for better model stability, which should improve performance by addressing skewness in the data.

```{r}
summary(insurance_training$AGE)
summary(insurance_training$CAR_AGE)
summary(insurance_training$KIDSDRIV)
summary(insurance_training$HOMEKIDS)
```
```{r}
any(is.na(insurance_training$AGE))
any(is.na(insurance_training$CAR_AGE))
any(is.na(insurance_training$KIDSDRIV))
any(is.na(insurance_training$HOMEKIDS))
```


```{r}
# Multiple Linear Regression - Model 2 (using log-transformed variables)
model2 <- lm(TARGET_AMT_LOG ~ AGE + CAR_AGE + KIDSDRIV_LOG + HOMEKIDS_LOG, 
             data = insurance_training)
summary(model2)
```

The individual predictors (AGE, CAR_AGE, KIDSDRIV_LOG, and HOMEKIDS_LOG) are statistically significant and have the expected signs in terms of their effect on the target variable (TARGET_AMT_LOG).

However, the model fit is weak (with a low R-squared of 0.02669), indicating that these predictors alone do not explain much of the variability in the target variable. There could be other variables or interactions that are not accounted for, or the relationship between predictors and the target may not be linear.

#### 3.1.3 Model 3: Using Interaction Terms

We introduce interaction terms between variables to explore the combined effects of variables on the target.

```{r}
# Multiple Linear Regression - Model 3 (including interaction terms)
model3 <- lm(TARGET_AMT_LOG ~ AGE * CAR_AGE + KIDSDRIV_LOG * HOMEKIDS_LOG, data = insurance_training)
summary(model3)
```

Adding interaction terms did not substantially improve the model fit. The main effects (AGE, CAR_AGE, KIDSDRIV_LOG, and HOMEKIDS_LOG) remain statistically significant, but the interaction terms are not, suggesting that these interactions do not improve the model's ability to predict TARGET_AMT_LOG.

Model fit is still weak with low R-squared values, meaning the model is not explaining much of the variability in the target. Further steps could include adding additional predictors or exploring non-linear relationships, or possibly using more advanced models such as random forests or boosting.


### 3.2. Binary Logistic Regression Models:

Binary logistic regression models predict a binary outcome (0 or 1). We'll predict the TARGET_FLAG (whether a claim happened: 0 = no, 1 = yes).

#### 3.2.1 Model 1: Using Transformed Variables

We’ll start by using some of the transformed variables that are more normally distributed, as they were transformed to reduce skewness and stabilize the model.

```{r}
# Logistic Regression - Model 1
log_model1 <- glm(TARGET_FLAG ~ AGE + CAR_AGE + KIDSDRIV_LOG + HOMEKIDS_LOG, 
                  family = binomial(link = "logit"), data = insurance_training)
summary(log_model1)

# Check multicollinearity using VIF
vif(log_model1)
```

AGE and CAR_AGE both have a negative relationship with the probability of TARGET_FLAG = 1, meaning as age and car age increase, the likelihood of the target outcome decreases.

KIDSDRIV_LOG and HOMEKIDS_LOG both have positive relationships with the target outcome, meaning that as these variables increase, the likelihood of TARGET_FLAG = 1 increases.

The model's fit is acceptable, but there is room for improvement, as indicated by the residual deviance and AIC.

```{r}
# Apply log transformation to variables in the evaluation dataset
insurance_evaluation$KIDSDRIV_LOG <- log(insurance_evaluation$KIDSDRIV + 1)
insurance_evaluation$HOMEKIDS_LOG <- log(insurance_evaluation$HOMEKIDS + 1)
```

**- Evaluating the model:**

```{r}
# Residual diagnostics
par(mfrow = c(2, 2))
plot(log_model1)

# Predict on training and evaluation datasets
insurance_training$PRED_TARGET_AMT <- exp(predict(log_model1, newdata = insurance_training)) - 1
insurance_evaluation$PRED_TARGET_AMT <- exp(predict(log_model1, newdata = insurance_evaluation)) - 1
```


#### 3.2.2 Model 2: Including Interaction Terms

We include interaction terms to explore the effect of variable combinations on the target variable.

```{r}
# Logistic Regression - Model 3 (including interaction terms + KIDSDRIV_RATIO)
log_model2 <- glm(TARGET_FLAG ~ AGE * CAR_AGE + KIDSDRIV_RATIO + HOMEKIDS_LOG, 
                  family = binomial(link = "logit"), data = insurance_training)
summary(log_model2)
```

The significant predictors in this model are AGE, KIDSDRIV_RATIO, and HOMEKIDS_LOG, indicating they are important in predicting the outcome (TARGET_FLAG).

The model is not greatly improved by the interaction term (AGE:CAR_AGE), suggesting that there is no strong interaction effect between AGE and CAR_AGE.

The CAR_AGE predictor is marginally significant, suggesting a potential relationship, but it is not as strong as the other variables.

#### 3.3.2 Model 3: Including Interaction Terms+ Other

```{r}
# Logistic Regression - Model 3 (Including Interaction Terms + KIDSDRIV_RATIO)
log_model3 <- glm(TARGET_FLAG ~ AGE * CAR_AGE + KIDSDRIV_RATIO + HOMEKIDS_LOG, 
                          family = binomial(link = "logit"), data = insurance_training)
summary(log_model3)
```

AGE and KIDSDRIV_RATIO are the strongest predictors, with KIDSDRIV_RATIO having a particularly large effect on the outcome.

CAR_AGE has a weaker, marginally significant effect, while HOMEKIDS_LOG also contributes significantly to the model.

The interaction between AGE and CAR_AGE does not significantly improve the model.


## 4. SELECT MODELS

In this section, we will evaluate the multiple linear regression and binary logistic regression models using various criteria. The goal is to select the models that provide the best balance between performance and interpretability, while also considering the business context and model simplicity. Here, we will explain the criteria used to select the best models, address potential issues such as multi-collinearity, and discuss the relevant model outputs.

### 4.1 Compare Coefficients:

The key objective for the multiple linear regression model is to find the best model that explains the variability in the target variable (TARGET_AMT_LOG).

Let's extract Coefficients and Standard Errors:

```{r}
# Model Evaluation for Multiple Linear Regression - Model 1
# Check for multicollinearity (VIF)
vif(model1)  # Variance Inflation Factor (VIF)

# Calculate R-squared, Adjusted R-squared, RMSE, and F-statistic
summary(model1)

# Plot residuals
par(mfrow = c(2, 2))
plot(model1)

# RMSE Calculation
rmse_model1 <- sqrt(mean(model1$residuals^2))

# Display results
cat("Adjusted R^2: ", summary(model1)$adj.r.squared, "\n")
cat("RMSE: ", rmse_model1, "\n")
cat("F-statistic: ", summary(model1)$fstatistic[1], "\n")
```
The model appears to have statistically significant predictors (with very low p-values), but the overall fit is poor as indicated by the low R-squared and adjusted R-squared values. This suggests that while individual predictors like age, car age, and home kids may have a significant relationship with the target variable, the model is not explaining much of the variability in the target variable. Further model refinement or additional predictors may be necessary for a better fit.

###  Calculate AIC and Adjusted R²:


```{r}
# Linear Models
coeff_model1 <- summary(model1)$coefficients
coeff_model2 <- summary(model2)$coefficients
coeff_model3 <- summary(model3)$coefficients

# Logistic Models
coeff_log_model1 <- summary(log_model1)$coefficients
coeff_log_model2 <- summary(log_model2)$coefficients
coeff_log_model3 <- summary(log_model3)$coefficients

# Display coefficients
print("Linear Model 1 Coefficients:")
coeff_model1

print("Linear Model 2 Coefficients:")
coeff_model2

print("Linear Model 3 Coefficients:")
coeff_model3

print("Logistic Model 1 Coefficients:")
coeff_log_model1

print("Logistic Model 2 Coefficients:")
coeff_log_model2
```


```{r}
# Linear Models
aic_model1 <- AIC(model1)
aic_model2 <- AIC(model2)  # Will be same as model1
aic_model3 <- AIC(model3)

adjusted_r2_model1 <- summary(model1)$adj.r.squared
adjusted_r2_model2 <- summary(model2)$adj.r.squared
adjusted_r2_model3 <- summary(model3)$adj.r.squared

# Logistic Models
aic_log_model1 <- AIC(log_model1)
aic_log_model2 <- AIC(log_model2)
aic_log_model3 <- AIC(log_model3)

# Display results
cat("Linear Models AIC and Adjusted R²:\n")
cat("Model 1: AIC =", aic_model1, "Adjusted R² =", adjusted_r2_model1, "\n")
cat("Model 2: AIC =", aic_model2, "Adjusted R² =", adjusted_r2_model2, "\n")
cat("Model 3: AIC =", aic_model3, "Adjusted R² =", adjusted_r2_model3, "\n")

cat("\nLogistic Models AIC:\n")
cat("Model 1: AIC =", aic_log_model1, "\n")
cat("Model 2: AIC =", aic_log_model2, "\n")
cat("Model 3: AIC =", aic_log_model3, "\n")
```

### Select Models Based on Metrics:

**Linear Regression Models**

- Model 1 and Model 2:

Both models are identical, as reflected by the same coefficients, AIC, and Adjusted R² values.
AIC: 44178.03
Adjusted R²: 0.0262

- Model 3:
Adds interaction terms (AGE:CAR_AGE and KIDSDRIV_LOG:HOMEKIDS_LOG).
Slightly higher Adjusted R² (0.0264) compared to Models 1 and 2.
Higher AIC (44178.84), suggesting Model 3 doesn't perform better overall.

- Decision for Linear Models:

Model 1 or Model 2 is preferred due to lower AIC, simpler structure, and comparable Adjusted R².

**Logistic Regression Models**

- Model 1: AIC: 9208.06;

Significant predictors: AGE, CAR_AGE, KIDSDRIV_LOG, HOMEKIDS_LOG (p-values < 0.05).

- Model 2: Adds AGE:CAR_AGE interaction and KIDSDRIV_RATIO. AIC: 9216.27 (higher than Model 1).

Significant predictors: AGE, KIDSDRIV_RATIO, and HOMEKIDS_LOG.

Interaction term AGE:CAR_AGE is not significant (p = 0.549), indicating no meaningful contribution.

Decision for Logistic Models:

Model 1 is preferred due to lower AIC and a more parsimonious structure.

So based on the above metrics and comparison, our final model selection is: Model1 for both linear regression and logistic regression.

Let's generate the ROC Curves for better decision:

```{r}
# Predict probabilities on the training dataset
probabilities <- predict(log_model1, newdata = insurance_training, type = "response")

# Calculate the ROC curve
roc_curve <- roc(insurance_training$TARGET_FLAG, probabilities)

# Plot the ROC curve
plot(roc_curve, col = "blue", lwd = 2, 
     main = "Corrected ROC Curve for Logistic Model 1",
     xlab = "False Positive Rate", ylab = "True Positive Rate", 
     xlim = c(0, 1), ylim = c(0, 1))  # Ensure proper axis limits
abline(a = 0, b = 1, lty = 2, col = "red")  # Add diagonal line

# Display the AUC
auc(roc_curve)
```

```{r}
# Predict probabilities on the training dataset
probabilities2 <- predict(log_model2, newdata = insurance_training, type = "response")

# Calculate the ROC curve
roc_curve2 <- roc(insurance_training$TARGET_FLAG, probabilities2)

# Plot the ROC curve
plot(roc_curve2, col = "blue", lwd = 2, 
     main = "Corrected ROC Curve for Logistic Model 1",
     xlab = "False Positive Rate", ylab = "True Positive Rate", 
     xlim = c(0, 1), ylim = c(0, 1))  # Ensure proper axis limits
abline(a = 0, b = 1, lty = 2, col = "red")  # Add diagonal line

# Display the AUC
auc(roc_curve2)
```

```{r}
# Predict probabilities on the training dataset
probabilities3 <- predict(log_model3, newdata = insurance_training, type = "response")

# Calculate the ROC curve
roc_curve3 <- roc(insurance_training$TARGET_FLAG, probabilities3)

# Plot the ROC curve
plot(roc_curve3, col = "blue", lwd = 2, 
     main = "Corrected ROC Curve for Logistic Model 1",
     xlab = "False Positive Rate", ylab = "True Positive Rate", 
     xlim = c(0, 1), ylim = c(0, 1))  # Ensure proper axis limits
abline(a = 0, b = 1, lty = 2, col = "red")  # Add diagonal line

# Display the AUC
auc(roc_curve3)
```


